# RAG Application with Qwen VLM

A complete Retrieval-Augmented Generation (RAG) application using Python, Streamlit, LangChain, and Qwen Vision-Language Model. Features multimodal support for text and image understanding.

![Python](https://img.shields.io/badge/Python-3.9+-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red)
![LangChain](https://img.shields.io/badge/LangChain-0.1+-green)
![License](https://img.shields.io/badge/License-MIT-green)

## Features

- **LangChain Integration**: Modern, composable RAG pipeline using LangChain
- **Knowledge Base Management**: Upload PDF, DOCX, TXT, and image files
- **Multimodal RAG**: Query with text and images
- **Document Selection**: Choose specific documents or use all
- **Chat Interface**: ChatGPT-style conversation with history
- **Source Citations**: Track which documents informed each answer
- **Streaming Responses**: Real-time token streaming
- **GPU/CPU Fallback**: Automatic hardware detection

## Prerequisites

### 1. Python 3.9+

Ensure you have Python 3.9 or newer installed.

### 2. Tesseract OCR (for image text extraction)

**Windows:**
1. Download installer from: https://github.com/UB-Mannheim/tesseract/wiki
2. Run the installer (default path: `C:\Program Files\Tesseract-OCR`)
3. Add to PATH or the app will auto-detect common paths

**Linux (Ubuntu/Debian):**
```bash
sudo apt update
sudo apt install tesseract-ocr
```

**macOS:**
```bash
brew install tesseract
```

### 3. GPU Support (Optional but Recommended)

For GPU acceleration, install PyTorch with CUDA support:
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

## Installation

1. **Clone or navigate to the project directory:**
```bash
cd rag_qwen_app
```

2. **Create a virtual environment (recommended):**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/macOS
source venv/bin/activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

## Model Download

The application uses these models (downloaded automatically on first run):

| Model | Purpose | Size |
|-------|---------|------|
| `all-MiniLM-L6-v2` | Text embeddings | ~80MB |
| `Qwen/Qwen2-VL-2B-Instruct` | Vision-Language Model | ~4GB |

**First run will take several minutes** while models download.

### Memory Requirements

| Mode | RAM | VRAM |
|------|-----|------|
| CPU Only | 8GB+ | - |
| GPU | 8GB+ | 6GB+ |

## Running the Application

```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

## Usage Guide

### 1. Upload Documents

1. Use the **left sidebar** to upload documents
2. Supported formats: PDF, DOCX, TXT, PNG, JPG, JPEG
3. Click **"Add to Knowledge Base"** to process

### 2. Select Context Documents

- **Toggle "Use all documents"**: Uses entire knowledge base
- **Or deselect the toggle**: Choose specific documents from the dropdown

### 3. Ask Questions

1. Type your question in the chat input
2. Optionally attach PDF/images using **"ğŸ“ Attach files"**
3. Press Enter to submit

### 4. View Responses

- Responses include inline citations
- Click **"ğŸ“š View Sources"** to see referenced documents
- Chat history is preserved during the session

## Project Structure

```
rag_qwen_app/
â”œâ”€â”€ app.py                     # Main Streamlit application
â”œâ”€â”€ langchain_wrappers/        # Custom LangChain wrappers
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ qwen_llm.py            # LangChain LLM for Qwen
â”‚   â””â”€â”€ clip_embeddings.py     # LangChain Embeddings for CLIP
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat_ui.py             # UI components
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loaders.py             # LangChain document loaders
â”‚   â”œâ”€â”€ ocr.py                 # Image OCR processor
â”‚   â””â”€â”€ chunking.py            # LangChain text splitters
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py            # HuggingFace embeddings
â”‚   â””â”€â”€ multimodal.py          # CLIP multimodal embeddings
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ store.py               # LangChain FAISS vector store
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pipeline.py            # LangChain RAG pipeline
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ qwen_vlm.py            # Qwen VLM integration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ file_utils.py          # File utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ knowledge_base/        # Uploaded documents
â”‚   â””â”€â”€ vector_store/          # FAISS index
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## LangChain Architecture

This application uses LangChain for a modern, composable RAG pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAGPipeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Loaders   â”‚  â”‚   Chunker   â”‚  â”‚  Embedder   â”‚         â”‚
â”‚  â”‚ (LangChain) â”‚â†’ â”‚ (LangChain) â”‚â†’ â”‚ (HuggingFace)â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â†“                                   â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              FAISS Vector Store                  â”‚       â”‚
â”‚  â”‚            (LangChain-compatible)                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Retriever  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚   QwenLLM   â”‚        â”‚
â”‚  â”‚             â”‚     context        â”‚ (LangChain) â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Custom LangChain Components

- **QwenLLM**: Custom LLM wrapper for Qwen VLM with multimodal support
- **CLIPEmbeddings**: Custom Embeddings for CLIP multimodal embeddings
- **HybridEmbeddings**: Combined text (Sentence-Transformers) + image (CLIP) embeddings

## Configuration

Key parameters can be adjusted in `app.py`:

```python
RAGPipeline(
    embedding_model_name="all-MiniLM-L6-v2",  # Embedding model
    chunk_size=512,                            # Characters per chunk
    chunk_overlap=50,                          # Overlap between chunks
    top_k=5                                    # Documents to retrieve
)
```

## Troubleshooting

### "Tesseract not found"
- Install Tesseract OCR (see Prerequisites)
- Or set path manually in `ingestion/ocr.py`

### "CUDA out of memory"
- The app will automatically fall back to CPU
- Or use a smaller model in `models/qwen_vlm.py`

### "Module not found"
- Ensure you're in the `rag_qwen_app` directory
- Check virtual environment is activated
- Install LangChain: `pip install langchain langchain-community langchain-core`

### Slow first query
- Normal: Model is loading on first query
- Subsequent queries will be faster

### "LangChain not found"
- Install LangChain dependencies:
```bash
pip install langchain langchain-community langchain-core langchain-huggingface
```

## Migration from Previous Version

If you're upgrading from the non-LangChain version:

1. **Install new dependencies**: `pip install -r requirements.txt`
2. **Re-index documents**: The vector store format is unchanged, but for best results, re-upload your documents
3. **API compatibility**: The `RAGPipeline` API remains the same

## License

MIT License - See LICENSE file for details.
